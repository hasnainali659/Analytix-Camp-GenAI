{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c8cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "path = \"Docs/plan_and_solve.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc3ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=25000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "splitted_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d557a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available!\n",
      "Loading BGE model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repos\\Projects\\Analytix Camp GenAI\\Analytix-Camp-GenAI\\analytix_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "\n",
    "def bge_embedding():\n",
    "    model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "    print(\"CUDA is available!\" if model_kwargs[\"device\"] == \"cuda\" else \"CUDA is not available!\")\n",
    "    print(\"Loading BGE model...\")\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "    embeddings = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "embeddings = bge_embedding()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1de9155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "import uuid\n",
    "\n",
    "collection_name = \"analytix_camp\"\n",
    "url = \"http://localhost:6333\"\n",
    "\n",
    "file_uuid = uuid.uuid4()\n",
    "\n",
    "documents_with_payload = []\n",
    "\n",
    "for chunk in splitted_docs:\n",
    "    chunk.metadata[\"group_id\"] = \"Hasnain Ali Poonja\"\n",
    "    chunk.metadata['file_uid'] = file_uuid\n",
    "    chunk.metadata['num_of_pages'] = len(docs)\n",
    "    \n",
    "    documents_with_payload.append(chunk)\n",
    "\n",
    "vector_store = Qdrant.from_documents(\n",
    "    documents_with_payload,\n",
    "    embeddings,\n",
    "    collection_name=collection_name,\n",
    "    url=url,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606b25f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=2, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "url = \"http://localhost:6333\"\n",
    "qdrant_client = QdrantClient(url=url)\n",
    "\n",
    "qdrant_client.delete(\n",
    "    collection_name=\"analytix_camp\",\n",
    "    points_selector=models.FilterSelector(\n",
    "        filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"metadata.file_uid\",\n",
    "                    match=models.MatchValue(value=\"d03e344a-6bba-4146-b0fc-dc17cf541179\"),\n",
    "                ),\n",
    "                models.FieldCondition(\n",
    "                    key=\"metadata.group_id\",\n",
    "                    match=models.MatchValue(value=\"Hasnain Ali Poonja\")\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3662de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f91c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasnainali659\\AppData\\Local\\Temp\\ipykernel_20152\\3574744268.py:21: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.1.2 and will be removed in 0.5.0. Use :class:`~QdrantVectorStore` instead.\n",
      "  vector_store = Qdrant(\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import os\n",
    "\n",
    "def build_file_conditions(file_uuid_array):\n",
    "    must_conditions = []\n",
    "    # file_uuid = \"1d4ae57f-d8fa-464c-8c2a-0eac67d340bd\"\n",
    "    \n",
    "    for file_uuid in file_uuid_array:\n",
    "        must_conditions.extend([\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.file_uid\",\n",
    "                match=models.MatchValue(value=file_uuid)\n",
    "            ),\n",
    "        ])\n",
    "    return must_conditions\n",
    "\n",
    "def create_vector_store_retriever(file_uuid_array, embeddings, collection_name):\n",
    "    url = \"http://localhost:6333\"\n",
    "    qdrant_client = QdrantClient(url=url)\n",
    "\n",
    "    vector_store = Qdrant(\n",
    "        client=qdrant_client,\n",
    "        collection_name=collection_name,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "\n",
    "    if file_uuid_array[0] != '':\n",
    "        file_conditions = build_file_conditions(file_uuid_array)\n",
    "        search_args = {\"filter\": models.Filter(should=file_conditions), \"k\": 5, \"score_threshold\": 0.4}\n",
    "        return vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs=search_args)\n",
    "    else:\n",
    "        search_args = {\"filter\": models.Filter(must=[models.FieldCondition(key=\"metadata.group_id\", match=models.MatchValue(value=\"Hasnain Ali Poonja\"))]), \"k\": 5, \"score_threshold\": 0.4}\n",
    "        return vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs=search_args)\n",
    "\n",
    "\n",
    "\n",
    "retriever = create_vector_store_retriever([\"1d4ae57f-d8fa-464c-8c2a-0eac67d340bd\"], embeddings, \"analytix_camp\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b0d842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_retrieved =  retriever.invoke(\"Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023. Describe, explain, plan and se- lect: Interactive planning with large language models enables open-world multi-task agents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75645471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-05-29T00:17:24+00:00', 'author': '', 'keywords': '', 'moddate': '2023-05-29T00:17:24+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/plan_and_solve.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11', 'group_id': 'Hasnain Ali Poonja', 'file_uid': '1d4ae57f-d8fa-464c-8c2a-0eac67d340bd', 'num_of_pages': 24, '_id': '8106b87c-b9b9-47a7-8ecd-6aadb8f20a9a', '_collection_name': 'analytix_camp'}, page_content='Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and\\nYitao Liang. 2023. Describe, explain, plan and se-\\nlect: Interactive planning with large language models\\nenables open-world multi-task agents. arXiv preprint\\narXiv:2302.01560.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\\n2022a. Emergent abilities of large language models.\\narXiv preprint arXiv:2206.07682.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. In Thirty-sixth Conference on Neu-\\nral Information Processing Systems (NeurIPS 2022).\\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu,\\nand Jun Zhao. 2022. Large language models are\\nreasoners with self-verification. arXiv preprint\\narXiv:2212.09561.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L. Griffiths, Yuan Cao, and Karthik\\nNarasimhan. 2023. Tree of thoughts: Deliberate\\nproblem solving with large language models.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\\nReact: Synergizing reasoning and acting in language\\nmodels. ArXiv, abs/2210.03629.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\\nSmola. 2022. Automatic chain of thought prompt-\\ning in large language models. arXiv preprint\\narXiv:2210.03493.\\nDenny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\\nLeast-to-most prompting enables complex reason-\\ning in large language models. arXiv preprint\\narXiv:2205.10625.\\nA Appendix\\nThis section includes two parts: (1) Results of all\\nprompts we have tried; (2) Example texts generated\\nby Zero-shot-PS+. Unless otherwise mentioned,\\nwe use GPT3 (text-davinci-003) model.\\nA.1 Results of All Trigger Sentences\\nTables 7 to 16 list the results of all prompts we have\\ntried for each dataset.\\nA.2 Example Outputs by Zero-shot-PS+\\nTables 17 to 25 list example outputs generated by\\nZero-shot-PS+ for each dataset.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-05-29T00:17:24+00:00', 'author': '', 'keywords': '', 'moddate': '2023-05-29T00:17:24+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/plan_and_solve.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10', 'group_id': 'Hasnain Ali Poonja', 'file_uid': '1d4ae57f-d8fa-464c-8c2a-0eac67d340bd', 'num_of_pages': 24, '_id': '9c9a010d-a7f9-40db-a6d6-28f8982cd9e5', '_collection_name': 'analytix_camp'}, page_content='Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\\nBruna Morrone, Quentin De Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\\nParameter-efficient transfer learning for nlp. In In-\\nternational Conference on Machine Learning, pages\\n2790‚Äì2799. PMLR.\\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\\nwards reasoning in large language models: A survey.\\narXiv preprint arXiv:2212.10403.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\\nIgor Mordatch. 2022. Language models as zero-shot\\nplanners: Extracting actionable knowledge for em-\\nbodied agents. In International Conference on Ma-\\nchine Learning, pages 9118‚Äì9147. PMLR.\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\\nharwal. 2022. Decomposed prompting: A modular\\napproach for solving complex tasks. arXiv preprint\\narXiv:2210.02406.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\\nguage models are zero-shot reasoners. arXiv preprint\\narXiv:2205.11916.\\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\\n2015. Parsing algebraic word problems into equa-\\ntions. Transactions of the Association for Computa-\\ntional Linguistics, 3:585‚Äì597.\\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\\nKushman, and Hannaneh Hajishirzi. 2016. MAWPS:\\nA math word problem repository. In Proceedings of\\nNAACL, pages 1152‚Äì1157.\\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\\nJian-Guang Lou, and Weizhu Chen. 2022. On the\\nadvance of making language models better reasoners.\\narXiv preprint arXiv:2206.02336.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 158‚Äì167.\\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,\\nShiqi Zhang, Joydeep Biswas, and Peter Stone.\\n2023. Llm+ p: Empowering large language mod-\\nels with optimal planning proficiency. arXiv preprint\\narXiv:2304.11477.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nproach. arXiv preprint arXiv:1907.11692.\\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\\nand Ashwin Kalyan. 2022. Dynamic prompt learning\\nvia policy gradient for semi-structured mathematical\\nreasoning. arXiv preprint arXiv:2209.14610.\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\\n2021. Are NLP models really able to solve simple\\nmath word problems? In Proceedings of NAACL,\\npages 2080‚Äì2094.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2022. Measuring\\nand narrowing the compositionality gap in language\\nmodels. arXiv preprint arXiv:2210.03350.\\nSubhro Roy and Dan Roth. 2016. Solving gen-\\neral arithmetic word problems. arXiv preprint\\narXiv:1608.01413.\\nAbulhair Saparov and He He. 2022. Language models\\nare greedy reasoners: A systematic formal analysis of\\nchain-of-thought. arXiv preprint arXiv:2210.01240.\\nOmar Shaikh, Hongxin Zhang, William Held, Michael\\nBernstein, and Diyi Yang. 2022. On second thought,\\nlet‚Äôs not think step by step! bias and toxicity in zero-\\nshot reasoning. arXiv preprint arXiv:2212.08061.\\nSimeng Sun, Yang Liu, Shuohang Wang, Chenguang\\nZhu, and Mohit Iyyer. 2023. Pearl: Prompting large\\nlanguage models to plan and execute actions over\\nlong documents.\\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\\nHuang, and Xipeng Qiu. 2022. Black-box tuning\\nfor language-model-as-a-service. arXiv preprint\\narXiv:2201.03514.\\nMirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Se-\\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\\nZhou, et al. 2022. Challenging big-bench tasks and\\nwhether chain-of-thought can solve them. arXiv\\npreprint arXiv:2210.09261.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. Commonsenseqa: A question\\nanswering challenge targeting commonsense knowl-\\nedge. In Proceedings of NAACL-HLT, pages 4149‚Äì\\n4158.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applica-\\ntions. arXiv preprint arXiv:2201.08239.\\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2022a.\\nTowards understanding chain-of-thought prompting:\\nAn empirical study of what matters. arXiv preprint\\narXiv:2212.10001.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\\nEd Chi, and Denny Zhou. 2022b. Self-consistency\\nimproves chain of thought reasoning in language\\nmodels. arXiv preprint arXiv:2203.11171.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-05-29T00:17:24+00:00', 'author': '', 'keywords': '', 'moddate': '2023-05-29T00:17:24+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/plan_and_solve.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1', 'group_id': 'Hasnain Ali Poonja', 'file_uid': '1d4ae57f-d8fa-464c-8c2a-0eac67d340bd', 'num_of_pages': 24, '_id': '69076b96-fd2e-4e68-9391-e8a8d8a2e1a0', '_collection_name': 'analytix_camp'}, page_content='Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought\\nReasoning by Large Language Models\\nLei Wang1 Wanyu Xu2 Yihuai Lan Zhiqiang Hu3 Yunshi Lan4\\nRoy Ka-Wei Lee3 Ee-Peng Lim1‚àó\\n1Singapore Management University\\n2Southwest Jiaotong University\\n3Singapore University of Technology and Design\\n4East China Normal University\\nAbstract\\nLarge language models (LLMs) have recently\\nbeen shown to deliver impressive performance\\nin various NLP tasks. To tackle multi-step rea-\\nsoning tasks, few-shot chain-of-thought (CoT)\\nprompting includes a few manually crafted\\nstep-by-step reasoning demonstrations which\\nenable LLMs to explicitly generate reasoning\\nsteps and improve their reasoning task accu-\\nracy. To eliminate the manual effort, Zero-\\nshot-CoT concatenates the target problem state-\\nment with ‚ÄúLet‚Äôs think step by step‚Äù as an in-\\nput prompt to LLMs. Despite the success of\\nZero-shot-CoT, it still suffers from three pit-\\nfalls: calculation errors, missing-step errors,\\nand semantic misunderstanding errors. To ad-\\ndress the missing-step errors, we propose Plan-\\nand-Solve (PS) Prompting. It consists of two\\ncomponents: first, devising a plan to divide the\\nentire task into smaller subtasks, and then car-\\nrying out the subtasks according to the plan.\\nTo address the calculation errors and improve\\nthe quality of generated reasoning steps, we\\nextend PS prompting with more detailed in-\\nstructions and derive PS+ prompting. We eval-\\nuate our proposed prompting strategy on ten\\ndatasets across three reasoning problems. The\\nexperimental results over GPT-3 show that our\\nproposed zero-shot prompting consistently out-\\nperforms Zero-shot-CoT across all datasets by\\na large margin, is comparable to or exceeds\\nZero-shot-Program-of-Thought Prompting, and\\nhas comparable performance with 8-shot CoT\\nprompting on the math reasoning problem. The\\ncode can be found at https://github.com/AGI-\\nEdgerunners/Plan-and-Solve-Prompting.\\n1 Introduction\\nLarge language models (LLMs) (Brown et al.,\\n2020; Thoppilan et al., 2022; Chowdhery et al.,\\n2022) have recently proven highly effective in var-\\nious NLP tasks. Unlike the previous pre-trained\\nlanguage models (PTMs) (Devlin et al., 2019; Liu\\n‚àóCorresponding author.\\n5\\n10\\n15\\n20\\n25\\n30\\n35Ratio (%)\\n7\\n12\\n27\\nCalculation Error\\nStep Missing Error\\nSemantic Misunderstanding\\nFigure 1: Error analysis of 46 GSM8K problems with in-\\ncorrect answers returned by Zero-shot-CoT using GPT-\\n3 LLM. Following Wei et al. (2022b) and Wang et al.\\n(2022a), we assign ‚ÄúCalculation Error‚Äù (7%), ‚ÄúStep\\nMissing Error‚Äù (12%), or ‚ÄúSemantic misunderstanding\\nError‚Äù (27%) to each incorrect answer.\\net al., 2019), these LLMs are typically provided as\\na service, with no access to model parameters due\\nto commercial considerations and potential risks of\\nmisuse (Sun et al., 2022). Thus, it is challenging\\nto fine-tune LLMs for downstream tasks (He et al.,\\n2021; Houlsby et al., 2019; Devlin et al., 2019).\\nInstead, we leverage LLMs to solve complex rea-\\nsoning problems by eliciting their strong reasoning\\nabilities over their embedded knowledge using in-\\nstructions (or trigger sentences). So far, LLMs have\\nshown impressive abilities to solve new reasoning\\nproblems by simply conditioning them on a few\\nillustrative examples (i.e., few-shot learning) or a\\nprompt to solve new problems without illustrative\\nexamples (i.e., zero-shot learning).\\nTo tackle multi-step complex reasoning tasks us-\\ning LLMs, Wei et al. (2022b) proposes few-shot\\nchain-of-thought (CoT) prompting, which enables\\nLLMs to explicitly generate the intermediate rea-\\nsoning steps before predicting the final answer with\\na few manual step-by-step reasoning demonstra-\\ntion examples. In (Kojima et al., 2022), Zero-shot\\nCoT eliminates the need for manually crafted ex-\\namples in prompts by appending ‚ÄúLet‚Äôs think step\\nby step‚Äù to the target problem fed to LLMs such\\narXiv:2305.04091v3  [cs.CL]  26 May 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-05-29T00:17:24+00:00', 'author': '', 'keywords': '', 'moddate': '2023-05-29T00:17:24+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/plan_and_solve.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8', 'group_id': 'Hasnain Ali Poonja', 'file_uid': '1d4ae57f-d8fa-464c-8c2a-0eac67d340bd', 'num_of_pages': 24, '_id': 'ce9e145f-78f3-4836-b99f-2fcc695e0865', '_collection_name': 'analytix_camp'}, page_content='CalculationStep Missing Semantic\\nVariablesPlanSolution\\n-0.41 -0.56 0.76\\n-0.02 -0.83 0.7\\n-0.42 0.076 0.24\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n0.2\\n0.4\\n0.6\\nFigure 5: Correlation analysis of generated reasoning\\nand error types of randomly sampled 100 data examples\\nfrom GSM8K for Zero-shot-PS+.\\nof strong planning abilities in recent LLMs such as\\nGPT-3.5 and GPT-4.\\n5 Related Work\\n5.1 Reasoning in NLP\\nIt is well known that complex reasoning prob-\\nlems are challenging for NLP models, and such\\nproblems include mathematical reasoning (Cobbe\\net al., 2021; Patel et al., 2021; Ling et al., 2017;\\nKoncel-Kedziorski et al., 2016) (requiring the abil-\\nity to understand mathematical concepts, calcu-\\nlation, and multi-step reasoning), commonsense\\nreasoning (Talmor et al., 2019; Geva et al., 2021)\\n(requiring the ability to make judgments based\\non commonsense knowledge), and logical reason-\\ning (Wei et al., 2022b) (requiring the ability to\\nmanipulate symbols by applying formal logical\\nrules). Before the advent of Large Language mod-\\nels (LLMs), Talmor et al. (2019) trained the NLP\\nmodel using explanations generated by the fine-\\ntuned GPT model and found that the trained model\\nyields better performance on commonsense QA\\nproblems. Hendrycks et al. (2021) attempted\\nto fine-tune pretrained language models with la-\\nbeled rationale, but found out that these fine-tuned\\nmodels could not easily generate high-quality rea-\\nsoning steps. Recent work by Wei et al. (2022a)\\nshowed that LLMs demonstrates strong reasoning\\nability when scaled up to tens of billions of pa-\\nrameters, such as GPT-3 (Brown et al., 2020) and\\nPaLM (Chowdhery et al., 2022). These LLMs with\\na few demonstration exemplars can yield impres-\\nsive performance across different NLP tasks. How-\\never, these models still perform poorly in problems\\nthat require multi-step reasoning. This may be due\\nto the fact that the few exemplars provided are in-\\nsufficient to unlock the LLMs‚Äô capabilities.\\n5.2 Prompting Methods\\nTo exploit the reasoning ability in LLMs, Wei\\net al. (2022b) propose Chain-of-Thought prompt-\\ning, appending multiple reasoning steps before the\\nanswer to the input question. With this simple\\nfew-shot prompting strategy, LLMs are able to per-\\nform much better in complex reasoning problems.\\nSubsequently, many works (Wang et al., 2022a;\\nSuzgun et al., 2022; Shaikh et al., 2022; Saparov\\nand He, 2022) propose to further improve CoT\\nprompting in different aspects, including prompt\\nformat (Chen et al., 2022), prompt selection (Lu\\net al., 2022), prompt ensemble (Wang et al., 2022b;\\nLi et al., 2022; Weng et al., 2022; Fu et al., 2022),\\nproblem decomposition (Zhou et al., 2022; Khot\\net al., 2022; Dua et al., 2022; Press et al., 2022),\\nand planning (Yao et al., 2022; Huang et al., 2022;\\nWang et al., 2023; Liu et al., 2023; Sun et al., 2023;\\nYao et al., 2023). Chen et al. (2022) introduced\\nPoT prompting to use LLMs with code pre-training\\nto write a program as a rationale for disentangling\\ncomputation from reasoning. To do away with man-\\nual effort, Kojima et al. (2022) proposed Zero-shot-\\nCoT to elicit reasoning step generation without\\nexemplars. To leverage the benefit of demonstra-\\ntion examples and minimize manual effort, Zhang\\net al. (2022) designed Auto-CoT. It first automat-\\nically obtains k examples by clustering the given\\ndataset. It then follows Zero-shot-CoT to gener-\\nate rationales for the selected examples. Finally,\\ndemonstration examples are constructed by adding\\nthe generated rationales to selected examples as\\nCoT prompts. Our work is different from the above\\nworks by focusing on eliciting multi-step reasoning\\nby LLMs in a zero-shot approach. We ask LLMs\\nto write a plan to decompose a complex reasoning\\ntask into multiple reasoning steps. Furthermore,\\nwe introduce detailed instructions to the prompt to\\navoid obvious errors in the reasoning steps. We re-\\nfer readers to the survey (Huang and Chang, 2022)\\nfor more related works.\\n6 Conclusion\\nIn this paper, we find that Zero-shot-CoT still suf-\\nfers from three pitfalls: calculation errors, missing-\\nreasoning-step errors, and semantic understand-\\ning errors. To address these issues, we introduce\\nplan-and-solve prompting strategies (PS and PS+\\nprompting). They are new zero-shot prompting\\nmethods that guide LLMs to devise a plan that di-\\nvides the entire task into smaller subtasks and then'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-05-29T00:17:24+00:00', 'author': '', 'keywords': '', 'moddate': '2023-05-29T00:17:24+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/plan_and_solve.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9', 'group_id': 'Hasnain Ali Poonja', 'file_uid': '1d4ae57f-d8fa-464c-8c2a-0eac67d340bd', 'num_of_pages': 24, '_id': '99c289d9-b44b-4cd3-a1d1-7e096fc13f5d', '_collection_name': 'analytix_camp'}, page_content='carries out the subtasks according to the plan. Eval-\\nuation on ten datasets across three types of reason-\\ning problems shows PS+ prompting outperforms\\nthe previous zero-shot baselines and performs on\\npar with few-shot CoT prompting on multiple arith-\\nmetic reasoning datasets. Overall, our results sug-\\ngest that (a) Zero-shot PS+ prompting can generate\\na high-quality reasoning process than Zero-shot-\\nCoT prompting since the PS prompts can provide\\nmore detailed instructions guiding the LLMs to per-\\nform correct reasoning; (b) Zero-shot PS+ prompt-\\ning has the potential to outperform manual Few-\\nshot CoT prompting, which hopefully will spark\\nfurther development of new CoT prompting ap-\\nproaches to elicit reasoning in LLMs. Moreover,\\nPS(+) prompting is a general idea that can be used\\nfor non-reasoning tasks, and refining the plan is\\nalso an interesting idea. We leave them for future\\nwork.\\n7 Limitations\\nThere are two limitations to this work. First, it takes\\neffort to design the prompt to guide the LLMs to\\ngenerate correct reasoning steps. The GPT-3 mod-\\nels are sensitive to the expressions in prompts. Thus\\nwe need to carefully design the prompts. Second,\\nthe proposed plan-and-solve prompting can help ad-\\ndress the calculation errors and missing-reasoning-\\nstep errors, but the semantic misunderstanding er-\\nrors still remain. We will explore how to address\\nsemantic misunderstanding errors by prompting\\ninstead of upgrading LLMs in the future.\\n8 Ethics\\nWe experiment on six math reasoning datasets, in-\\ncluding AQuA (Ling et al., 2017), GSM8K (Cobbe\\net al., 2021), MultiArith, AddSub, SingleEq, and\\nSV AMP (Patel et al., 2021), two commonsense\\nreasoning tasks (CommonsenseQA (Talmor et al.,\\n2019) and StrategyQA (Geva et al., 2021)), and\\ntwo symbolic tasks (Last Letter and Coin Flip (Wei\\net al., 2022b)), where GSM8K and SV AMP use\\nthe MIT License code, AQUA and StrategyQA use\\nthe Apache-2.0 code, the remaining datasets are\\nunspecified.\\nThe proposed prompts do not collect and use\\npersonal information about other individuals. The\\nprompts we used are listed in Appendix. The\\nprompts in this work do not contain any words\\nthat discriminate against any individual or group.\\nIn this work, prompts would not negatively impact\\nother people‚Äôs safety.\\nReferences\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877‚Äì1901.\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\\nWilliam W Cohen. 2022. Program of thoughts\\nprompting: Disentangling computation from reason-\\ning for numerical reasoning tasks. arXiv preprint\\narXiv:2211.12588.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. PaLM: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\\nian, Jacob Hilton, Reiichiro Nakano, Christopher\\nHesse, and John Schulman. 2021. Training veri-\\nfiers to solve math word problems. arXiv preprint\\narXiv:2110.14168.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of NAACL, pages 4171‚Äì\\n4186.\\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and\\nMatt Gardner. 2022. Successive prompting for\\ndecomposing complex questions. arXiv preprint\\narXiv:2212.04092.\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\\nand Tushar Khot. 2022. Complexity-based prompt-\\ning for multi-step reasoning. arXiv preprint\\narXiv:2210.00720.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. TACL, 9:346‚Äì361.\\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\\nKirkpatrick, and Graham Neubig. 2021. Towards a\\nunified view of parameter-efficient transfer learning.\\narXiv preprint arXiv:2110.04366.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\\ncob Steinhardt. 2021. Measuring mathematical prob-\\nlem solving with the math dataset. arXiv preprint\\narXiv:2103.03874.\\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\\nEtzioni, and Nate Kushman. 2014. Learning to solve\\narithmetic word problems with verb categorization.\\nIn EMNLP, pages 523‚Äì533.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8415a89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and\\nYitao Liang. 2023. Describe, explain, plan and se-\\nlect: Interactive planning with large language models\\nenables open-world multi-task agents. arXiv preprint\\narXiv:2302.01560.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\\n2022a. Emergent abilities of large language models.\\narXiv preprint arXiv:2206.07682.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. In Thirty-sixth Conference on Neu-\\nral Information Processing Systems (NeurIPS 2022).\\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu,\\nand Jun Zhao. 2022. Large language models are\\nreasoners with self-verification. arXiv preprint\\narXiv:2212.09561.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nThomas L. Griffiths, Yuan Cao, and Karthik\\nNarasimhan. 2023. Tree of thoughts: Deliberate\\nproblem solving with large language models.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\\nReact: Synergizing reasoning and acting in language\\nmodels. ArXiv, abs/2210.03629.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\\nSmola. 2022. Automatic chain of thought prompt-\\ning in large language models. arXiv preprint\\narXiv:2210.03493.\\nDenny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,\\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\\nLeast-to-most prompting enables complex reason-\\ning in large language models. arXiv preprint\\narXiv:2205.10625.\\nA Appendix\\nThis section includes two parts: (1) Results of all\\nprompts we have tried; (2) Example texts generated\\nby Zero-shot-PS+. Unless otherwise mentioned,\\nwe use GPT3 (text-davinci-003) model.\\nA.1 Results of All Trigger Sentences\\nTables 7 to 16 list the results of all prompts we have\\ntried for each dataset.\\nA.2 Example Outputs by Zero-shot-PS+\\nTables 17 to 25 list example outputs generated by\\nZero-shot-PS+ for each dataset.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_retrieved[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd373b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_context = \"\\n\\n\".join(doc.page_content for doc in documents_retrieved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ccdc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0a743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"What is the abstract of the paper?\"\n",
    "\n",
    "response = contextualize_q_chain.invoke({\"chat_history\": chat_history, \"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1a986f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you provide the abstract of the paper?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3f29a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"\n",
    "You are a highly accurate and question-answering RAG agent. Your task is to provide answers based on the given context.\n",
    "\n",
    "Instructions:\n",
    "1. Answer the users QUESTION using the CONTEXT text privided.Keep your answer ground in the facts of the CONTEXT.\n",
    "2. Determine if the context answers the question.\n",
    "3. If the answer is not found in the CONTEXT, respond with your own knowlege that can be outside of the CONTEXT but in that case start by saying this \"It seems that the information required to answer is not available in your documents, however, based on my knowledge, I can provide you with the following information:\".\n",
    "\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_chain = qa_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a309ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_response = rag_chain.invoke({\"chat_history\": chat_history, \"question\": query, \"context\": combined_context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f547c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The abstract of the paper is as follows:\\n\\n\"Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with ‚ÄúLet‚Äôs think step by step‚Äù as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.\"'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df298816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "\n",
    "qa_system_prompt = \"\"\"\n",
    "You are a highly accurate and question-answering RAG agent. Your task is to provide answers based on the given context.\n",
    "\n",
    "Instructions:\n",
    "1. Answer the users QUESTION using the CONTEXT text privided.Keep your answer ground in the facts of the CONTEXT.\n",
    "2. Determine if the context answers the question.\n",
    "3. If the answer is not found in the CONTEXT, respond with your own knowlege that can be outside of the CONTEXT but in that case start by saying this \"It seems that the information required to answer is not available in your documents, however, based on my knowledge, I can provide you with the following information:\".\n",
    "\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1752bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_chain(rag_chain, query, chat_history):\n",
    "    ai_response = \"\"\n",
    "\n",
    "    async for token in rag_chain.astream(\n",
    "        {\"question\": query, \"chat_history\": chat_history}\n",
    "    ):\n",
    "        yield token.content\n",
    "        ai_response += token.content\n",
    "\n",
    "    chat_history.extend(\n",
    "        [HumanMessage(content=query), AIMessage(content=ai_response)]\n",
    "    )\n",
    "\n",
    "    print(\"RAG chain executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5815dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that the information required to answer is not available in your documents, however, based on my knowledge, I can provide you with the following information: The abstract of a paper typically summarizes the main objectives, methods, results, and conclusions of the research. If you need the specific abstract of the paper mentioned in the context, you would need to access the paper directly from a database or repository where it is published, such as arXiv.RAG chain executed successfully.\n"
     ]
    }
   ],
   "source": [
    "async def test_streaming():\n",
    "    query = \"What is the abstract of the paper?\"\n",
    "    chat_history = []\n",
    "\n",
    "    async for token in run_chain(rag_chain, query, chat_history):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "await test_streaming()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytix_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
